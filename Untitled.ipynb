{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3183231f-c347-4abc-beb7-7863da27bbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mousky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Mousky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens : ['Le', 'chat', 'mange', 'une', 'souris', '.', 'Le', 'chien', 'joue', 'dans', 'le', 'jardin']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Exemple \n",
    "texte = \"Le chat mange une souris. Le chien joue dans le jardin\"\n",
    "tokens = word_tokenize(texte)\n",
    "\n",
    "print(\"Tokens :\", tokens )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "925894d8-090d-4c5c-8c1a-2ba0b4b22b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire : {'Le': 0, 'chat': 1, 'mange': 2, 'une': 3, 'souris': 4, '.': 5, 'chien': 6, 'joue': 7, 'dans': 8, 'le': 9, 'jardin': 10}\n",
      "Indices : [0, 1, 2, 3, 4, 5, 0, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# Construire le vocabulaire\n",
    "counter = Counter(tokens)\n",
    "vocab = {word: i for i, (word, _) in enumerate(counter.items())}\n",
    "print(\"Vocabulaire :\", vocab)\n",
    "\n",
    "# Convertir les tokens en indices\n",
    "indices = [vocab[token] for token in tokens]\n",
    "print(\"Indices :\", indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35dd9e2f-a18f-4a00-8b58-9636329e80a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrée (contexte) : tensor([[4, 5],\n",
      "        [5, 0]])\n",
      "Cible : tensor([0, 6])\n"
     ]
    }
   ],
   "source": [
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, indices, context_size=2):\n",
    "        self.data = []\n",
    "        for i in range(len(indices) - context_size):\n",
    "            self.data.append((indices[i:i+context_size], indices[i+context_size]))\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "   \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
    "\n",
    "# Paramètres\n",
    "context_size = 2  # Taille du contexte (2 mots précédents)\n",
    "dataset = LanguageDataset(indices, context_size)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Exemple de données\n",
    "for x, y in dataloader:\n",
    "    print(\"Entrée (contexte) :\", x)\n",
    "    print(\"Cible :\", y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82d40a09-cbc0-4418-a920-3cc016200ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleLM(\n",
      "  (embeddings): Embedding(11, 10)\n",
      "  (linear1): Linear(in_features=20, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=11, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, context_size):\n",
    "        super(SimpleLM, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear1 = nn.Linear(embed_dim * context_size, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.relu = nn.ReLU()\n",
    "   \n",
    "    def forward(self, x):\n",
    "        embeds = self.embeddings(x).view(x.size(0), -1)  # Embedding -> aplatissement\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# Initialisation du modèle\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 10\n",
    "model = SimpleLM(vocab_size, embed_dim, context_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af1dc7ea-7742-47e4-964c-a6b9ffe442da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contexte : ['le', 'chien'], Mot prédit : souris\n"
     ]
    }
   ],
   "source": [
    "def predict_next_word(context):\n",
    "    with torch.no_grad():\n",
    "        context_indices = torch.tensor([[vocab[word] for word in context]])\n",
    "        output = model(context_indices)\n",
    "        predicted_index = torch.argmax(output, dim=1).item()\n",
    "        for word, idx in vocab.items():\n",
    "            if idx == predicted_index:\n",
    "                return word\n",
    "\n",
    "# Exemple de prédiction\n",
    "context = ['le','chien']\n",
    "predicted_word = predict_next_word(context)\n",
    "print(f\"Contexte : {context}, Mot prédit : {predicted_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2441b922-1397-4da0-a11a-d8b678590cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 12:24:34.253 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Mousky\\anaconda3\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-12-19 12:24:34.255 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import streamlit as st\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Définir la variable d'environnement\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyCqozHPzc1NRb-Xf4t6DEYTDIutFcOe_bU\"\n",
    "\n",
    "# Configurer l'API Gemini\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 40,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-2.0-flash-exp\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# API de football (exemple d'URL, à adapter selon votre fournisseur d'API)\n",
    "FOOTBALL_API_KEY = os.getenv(\"734eef5ca3msh5d81108492b36d8p19753cjs\")  # Ajoutez votre clé API\n",
    "FOOTBALL_API_URL = \"https://api-football-v1.p.rapidapi.com/v2/odds/league/865927/bookmaker/5?page=2\"  # Exemple\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {FOOTBALL_API_KEY}\"}\n",
    "\n",
    "# Fonction pour récupérer les données des joueurs\n",
    "def get_player_stats(player_name):\n",
    "    \"\"\"Recherche les statistiques d'un joueur via une API de football.\"\"\"\n",
    "    response = requests.get(\n",
    "        f\"{FOOTBALL_API_URL}?search={player_name}\",\n",
    "        headers=headers,\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # Retourne les données sous forme JSON\n",
    "    else:\n",
    "        return {\"error\": \"Impossible de récupérer les données du joueur.\"}\n",
    "\n",
    "# Fonction pour générer une réponse avec Gemini\n",
    "def generate_response(prompt):\n",
    "    \"\"\"Utilise Gemini pour générer des réponses basées sur un prompt.\"\"\"\n",
    "    chat_session = model.start_chat(history=[])\n",
    "    response = chat_session.send_message(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Interface Streamlit\n",
    "st.title(\"LLM sur les nominés au Ballon d'Or\")\n",
    "st.markdown(\"Posez vos questions sur les nominés au Ballon d'Or et obtenez des réponses intelligentes !\")\n",
    "\n",
    "# Liste des nominés (vous pouvez enrichir ou lier à une API pour dynamiser cette partie)\n",
    "nominees = [\n",
    "    \"Lionel Messi\",\n",
    "    \"Erling Haaland\",\n",
    "    \"Kylian Mbappé\",\n",
    "    \"Kevin De Bruyne\",\n",
    "    \"Robert Lewandowski\",\n",
    "    \"Karim Benzema\",\n",
    "    \"Mohamed Salah\",\n",
    "]\n",
    "\n",
    "# Sélection du joueur\n",
    "player = st.selectbox(\"Sélectionnez un joueur :\", nominees)\n",
    "\n",
    "# Entrée de question\n",
    "question = st.text_input(\"Posez une question :\", placeholder=\"Exemple : Quelles sont ses statistiques cette saison ?\")\n",
    "\n",
    "# Actions\n",
    "if st.button(\"Obtenir une réponse\"):\n",
    "    if player and question:\n",
    "        # Récupération des données du joueur\n",
    "        stats = get_player_stats(player)\n",
    "\n",
    "        # Construction du prompt\n",
    "        prompt = f\"Voici les données disponibles pour {player} : {stats}. Maintenant, réponds à la question : {question}\"\n",
    "\n",
    "        # Génération de réponse avec Gemini\n",
    "        answer = generate_response(prompt)\n",
    "\n",
    "        # Affichage de la réponse\n",
    "        st.markdown(f\"**Réponse :** {answer}\")\n",
    "    else:\n",
    "        st.error(\"Veuillez sélectionner un joueur et poser une question.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a93da921-fcdc-4553-82cc-62817b86d112",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1021910414.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    streamlit run Untitled.ipynb\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "streamlit run Untitled.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
